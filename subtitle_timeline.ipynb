{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "subtitle_timeline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOAtuL8oUlRAl/tSuxJPNSd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/profteachkids/subtitle_generator/blob/main/subtitle_timeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqdi3Ry9vUZW",
        "outputId": "a0d26d75-5fb1-49ee-b9fe-b7c950b92445"
      },
      "source": [
        "!pip install biopython\n",
        "!pip install bidict\n",
        "!pip install torchaudio"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting biopython\n",
            "  Downloading biopython-1.79-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 11.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from biopython) (1.19.5)\n",
            "Installing collected packages: biopython\n",
            "Successfully installed biopython-1.79\n",
            "Collecting bidict\n",
            "  Downloading bidict-0.21.2-py2.py3-none-any.whl (37 kB)\n",
            "Installing collected packages: bidict\n",
            "Successfully installed bidict-0.21.2\n",
            "Collecting torchaudio\n",
            "  Downloading torchaudio-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 12.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchaudio) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchaudio) (3.7.4.3)\n",
            "Installing collected packages: torchaudio\n",
            "Successfully installed torchaudio-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jp919Vc00JF0",
        "outputId": "6f3437b8-b2ac-46be-8362-125430d3caea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget https://github.com/profteachkids/subtitle_generator/raw/main/en_v5.jit\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-05 12:01:02--  https://github.com/profteachkids/subtitle_generator/raw/main/en_v5.jit\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://media.githubusercontent.com/media/profteachkids/subtitle_generator/main/en_v5.jit [following]\n",
            "--2021-09-05 12:01:03--  https://media.githubusercontent.com/media/profteachkids/subtitle_generator/main/en_v5.jit\n",
            "Resolving media.githubusercontent.com (media.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to media.githubusercontent.com (media.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 117217582 (112M) [application/octet-stream]\n",
            "Saving to: ‘en_v5.jit’\n",
            "\n",
            "en_v5.jit           100%[===================>] 111.79M   245MB/s    in 0.5s    \n",
            "\n",
            "2021-09-05 12:01:03 (245 MB/s) - ‘en_v5.jit’ saved [117217582/117217582]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyMWoVjJvCHP"
      },
      "source": [
        "import numpy as np\n",
        "from lxml import etree\n",
        "import subprocess as sp\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from Bio import pairwise2\n",
        "from Bio.pairwise2 import format_alignment\n",
        "from bidict import bidict\n",
        "import string\n",
        "from glob import glob\n",
        "import re\n",
        "import shutil"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPyZqFgJ0Fjc"
      },
      "source": [
        "class Decoder():\n",
        "    def __init__(self,labels):\n",
        "        self.labels = labels\n",
        "        self.blank_idx = self.labels.index('_')\n",
        "        self.space_idx = self.labels.index(' ')\n",
        "\n",
        "    def process(self,\n",
        "                probs, wav_len, word_align):\n",
        "        assert len(self.labels) == probs.shape[1]\n",
        "        for_string = []\n",
        "        argm = torch.argmax(probs, axis=1)\n",
        "        align_list = [[]]\n",
        "        for j, i in enumerate(argm):\n",
        "            if i == self.labels.index('2'):\n",
        "                try:\n",
        "                    prev = for_string[-1]\n",
        "                    for_string.append('$')\n",
        "                    for_string.append(prev)\n",
        "                    align_list[-1].append(j)\n",
        "                    continue\n",
        "                except:\n",
        "                    for_string.append(' ')\n",
        "                    warnings.warn('Token \"2\" detected a the beginning of sentence, omitting')\n",
        "                    align_list.append([])\n",
        "                    continue\n",
        "            if i != self.blank_idx:\n",
        "                for_string.append(self.labels[i])\n",
        "                if i == self.space_idx:\n",
        "                    align_list.append([])\n",
        "                else:\n",
        "                    align_list[-1].append(j)\n",
        "\n",
        "        string = ''.join([x[0] for x in groupby(for_string)]).replace('$', '').strip()\n",
        "\n",
        "        align_list = list(filter(lambda x: x, align_list))\n",
        "\n",
        "        if align_list and wav_len and word_align:\n",
        "            align_dicts = []\n",
        "            linear_align_coeff = wav_len / len(argm)\n",
        "            to_move = min(align_list[0][0], 1.5)\n",
        "            for i, align_word in enumerate(align_list):\n",
        "                if len(align_word) == 1:\n",
        "                    align_word.append(align_word[0])\n",
        "                align_word[0] = align_word[0] - to_move\n",
        "                if i == (len(align_list) - 1):\n",
        "                    to_move = min(1.5, len(argm) - i)\n",
        "                    align_word[-1] = align_word[-1] + to_move\n",
        "                else:\n",
        "                    to_move = min(1.5, (align_list[i+1][0] - align_word[-1]) / 2)\n",
        "                    align_word[-1] = align_word[-1] + to_move\n",
        "\n",
        "            for word, timing in zip(string.split(), align_list):\n",
        "                align_dicts.append({'word': word,\n",
        "                                    'start_ts': round(timing[0] * linear_align_coeff, 2),\n",
        "                                    'end_ts': round(timing[-1] * linear_align_coeff, 2)})\n",
        "\n",
        "            return string, align_dicts\n",
        "        return string\n",
        "\n",
        "    def __call__(self,\n",
        "                 probs: torch.Tensor,\n",
        "                 wav_len: float = 0,\n",
        "                 word_align: bool = False):\n",
        "        return self.process(probs, wav_len, word_align)\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "model = torch.jit.load(\"en_v5.jit\", map_location=torch.device('cpu'))\n",
        "model.eval()\n",
        "decoder=Decoder(model.labels)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klBr8oQUvKwk"
      },
      "source": [
        "text = \"\"\"after the questionable\n",
        "events of the last part\n",
        "where Spadeless lost\n",
        "his pets\"\"\"\n",
        "\n",
        "text = re.sub(r\"[,.!?-] \", \" \", text, 0, re.MULTILINE)\n",
        "text = text.lower()\n",
        "text = text.split()\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdMYHhrEvhwQ"
      },
      "source": [
        "DEVNULL = open(os.devnull, \"w\")\n",
        "\n",
        "def ffmpeg_load_audio(\n",
        "    filename,\n",
        "    sr=44100,\n",
        "    mono=False,\n",
        "    normalize=True,\n",
        "    in_type=np.int16,\n",
        "    out_type=np.float32,\n",
        "):\n",
        "    channels = 1 if mono else 2\n",
        "    format_strings = {\n",
        "        np.float64: \"f64le\",\n",
        "        np.float32: \"f32le\",\n",
        "        np.int16: \"s16le\",\n",
        "        np.int32: \"s32le\",\n",
        "        np.uint32: \"u32le\",\n",
        "    }\n",
        "    format_string = format_strings[in_type]\n",
        "    command = [\n",
        "        \"ffmpeg\",\n",
        "        \"-i\",\n",
        "        filename,\n",
        "        \"-f\",\n",
        "        format_string,\n",
        "        \"-acodec\",\n",
        "        \"pcm_\" + format_string,\n",
        "        \"-ar\",\n",
        "        str(sr),\n",
        "        \"-ac\",\n",
        "        str(channels),\n",
        "        \"-\",\n",
        "    ]\n",
        "    p = sp.Popen(command, stdout=sp.PIPE, stderr=DEVNULL, bufsize=4096)\n",
        "    bytes_per_sample = np.dtype(in_type).itemsize\n",
        "    frame_size = bytes_per_sample * channels\n",
        "    chunk_size = frame_size * sr  # read in 1-second chunks\n",
        "    raw = b\"\"\n",
        "    with p.stdout as stdout:\n",
        "        while True:\n",
        "            data = stdout.read(chunk_size)\n",
        "            if data:\n",
        "                raw += data\n",
        "            else:\n",
        "                break\n",
        "    audio = np.fromstring(raw, dtype=in_type).astype(out_type)\n",
        "    if channels > 1:\n",
        "        audio = audio.reshape((-1, channels)).transpose()\n",
        "    if audio.size == 0:\n",
        "        return audio, sr\n",
        "    if issubclass(out_type, np.floating):\n",
        "        if normalize:\n",
        "            peak = np.abs(audio).max()\n",
        "            if peak > 0:\n",
        "                audio /= peak\n",
        "        elif issubclass(in_type, np.integer):\n",
        "            audio /= np.iinfo(in_type).max\n",
        "    return audio, sr\n",
        "\n",
        "def to_text(f):\n",
        "    print(f)\n",
        "    audio, sr = ffmpeg_load_audio(\n",
        "        f, sr=16000, in_type=np.float32, out_type=np.float32\n",
        "    )\n",
        "    audio = np.expand_dims(audio.astype(np.float32)[0], 0)\n",
        "\n",
        "    res = []\n",
        "    batch_size =  3*sr\n",
        "    N = audio.size // batch_size\n",
        "\n",
        "    sections = np.array_split(audio, N, 1)\n",
        "    section_start=0\n",
        "    for i, section in enumerate(sections):\n",
        "        input=torch.from_numpy(section)\n",
        "        output = model(input)[0]\n",
        "        decoded=decoder(output.cpu(), section.size, word_align=True)\n",
        "        if len(decoded)==2:\n",
        "            s,dlist = decoded\n",
        "            for d in dlist:\n",
        "                d[\"start_ts\"] = (d[\"start_ts\"] + section_start)/sr\n",
        "                d[\"end_ts\"] = (d[\"end_ts\"] +  section_start)/sr\n",
        "                print(d[\"word\"], d[\"start_ts\"], d[\"end_ts\"])\n",
        "                res.append(d)\n",
        "            section_start+=section.size\n",
        "\n",
        "\n",
        "    df = pd.DataFrame(res)\n",
        "    df.to_csv(\"voice_times.csv\")\n",
        "\n",
        "    voice_words = df[\"word\"].values\n",
        "    print(\" \".join(voice_words))\n",
        "\n",
        "def align():\n",
        "    df = pd.read_csv(\"voice_times.csv\")\n",
        "    voice_words = df[\"word\"].values\n",
        "    start_times = df[\"start_ts\"].values\n",
        "    end_times = df[\"end_ts\"].values\n",
        "\n",
        "    all_words = set(text).union(set(voice_words))\n",
        "\n",
        "    word_dict = bidict(zip(all_words, range(len(all_words))))\n",
        "    word_dict[\"-\"] = \"-\"\n",
        "    voice = [word_dict[word] for word in voice_words]\n",
        "    script = [word_dict[word] for word in text]\n",
        "    align = pairwise2.align.globalxx(voice, script, gap_char =['-'], one_alignment_only=True)[0]\n",
        "    matches=[]\n",
        "    mismatches=[]\n",
        "    mismatch_voice=word_dict.inverse[align.seqA[0]]   \n",
        "    mismatch_text=word_dict.inverse[align.seqB[0]]\n",
        "    v_end_pos=0\n",
        "    v_start_pos=0\n",
        "    t_end_pos=0\n",
        "    t_start_pos=0\n",
        "    for i,(v, t) in enumerate(zip(align.seqA[1:], align.seqB[1:])):\n",
        "        print(word_dict.inverse[v], word_dict.inverse[t])\n",
        "        if v==t:\n",
        "            mismatches.append([t_start_pos, t_end_pos, start_times[v_start_pos], end_times[v_end_pos], mismatch_voice, mismatch_text])\n",
        "            v_end_pos+=1\n",
        "            v_start_pos=v_end_pos\n",
        "            t_end_pos+=1\n",
        "            t_start_pos=t_end_pos\n",
        "            mismatch_voice=word_dict.inverse[v]\n",
        "            mismatch_text=word_dict.inverse[t]\n",
        "        else:\n",
        "            if word_dict.inverse[v] != '-' :\n",
        "                v_end_pos+=1\n",
        "            if word_dict.inverse[t] != '-' :\n",
        "                t_end_pos+=1\n",
        "            mismatch_voice+=' ' + word_dict.inverse[v]\n",
        "            mismatch_text+=' ' + word_dict.inverse[t]\n",
        "\n",
        "\n",
        "    mismatches.append([t_start_pos, t_end_pos, start_times[v_start_pos], end_times[v_end_pos], mismatch_voice, mismatch_text])\n",
        "\n",
        "    idx=[]\n",
        "    times=[]\n",
        "    for t_start_pos, t_end_pos, start_time, end_time, v, t in mismatches:\n",
        "        print('-'*30)\n",
        "        print(v)\n",
        "        print(t)\n",
        "        print(t_start_pos,t_end_pos, start_time, end_time)\n",
        "        d={}\n",
        "\n",
        "        t2 = t.translate(t.maketrans(string.punctuation, \" \" * len(string.punctuation)))\n",
        "        t2 = \" \".join(t2.split())\n",
        "\n",
        "        idx.append(t_start_pos)\n",
        "        times.append(start_time)\n",
        "\n",
        "    idx.append(t_end_pos+1)\n",
        "    times.append(end_time)\n",
        "\n",
        "    np.savez('it',idx=np.array(idx),time=np.array(times))\n",
        "    it = np.load('it.npz')\n",
        "    print(it['idx'], len(it['idx']))\n",
        "    print(it['time'], len(it['time']))\n",
        "\n",
        "def make_copies():\n",
        "    it = np.load('it.npz')\n",
        "    idx=it['idx']\n",
        "    times=it['time']\n",
        "    print(times, len(times))\n",
        "    path_out='E:\\\\Blender\\\\BlenderVideo\\\\'\n",
        "    path_in='E:\\\\Blender\\\\BlenderOut\\\\'\n",
        "\n",
        "    i=0\n",
        "    subtitle_images = glob(path_in+'*.png')\n",
        "    print(subtitle_images)\n",
        "    for subtitle_image in subtitle_images:\n",
        "        name, subtitle_n, w_start, w_end = subtitle_image.split('_')\n",
        "        *_, name = name.split('\\\\')\n",
        "        w_end,_ = w_end.split('.')\n",
        "        w_start, w_end, subtitle_n =int(w_start), int(w_end), int(subtitle_n)\n",
        "        t_start, t_end = np.interp([w_start, w_end], idx, times)\n",
        "        f_start, f_end = int(t_start*10), int(t_end*10)\n",
        "        print(name, w_start, w_end, t_start, t_end, f_start, f_end)\n",
        "        for copy_n in range(f_end-f_start+1):\n",
        "            \n",
        "            name=f'{i:04d}'.translate(str.maketrans('0123456789','abcdefghij'))\n",
        "            shutil.copyfile(subtitle_image, path_out+name+'.png')\n",
        "            i+=1\n",
        "\n",
        "def set_timeline():\n",
        "    it = np.load('it.npz')\n",
        "    idx=it['idx']\n",
        "    times=it['time']\n",
        "\n",
        "\n",
        "    tree = etree.parse('Timeline 1.xml')\n",
        "    clips=tree.xpath('//video//clipitem')\n",
        "    for clip in clips:\n",
        "        name=clip.xpath('name/text()')[0]\n",
        "        start=clip.xpath('start')[0]\n",
        "        end=clip.xpath('end')[0]\n",
        "        name, subtitle_n, w_start, w_end = name.split('_')\n",
        "        *_, name = name.split('\\\\')\n",
        "        w_end,_ = w_end.split('.')\n",
        "        w_start, w_end, subtitle_n =int(w_start), int(w_end), int(subtitle_n)\n",
        "        t_start, t_end = np.interp([w_start, w_end], idx, times)\n",
        "        start.text=str(int(t_start*60))\n",
        "        end.text=str(int(t_end*60))\n",
        "\n",
        "    f = open('subtitle_timeline.xml', 'wb')\n",
        "    f.write(etree.tostring(tree, pretty_print=True))\n",
        "    f.close()\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        },
        "id": "Wrcr7GBpv8dz",
        "outputId": "35db3f5d-ca37-4366-b77b-647cc3586160"
      },
      "source": [
        "to_text(\"audio.mov\")\n",
        "align()    \n",
        "set_timeline()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "audio.mov\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/shape_base.py\u001b[0m in \u001b[0;36marray_split\u001b[0;34m(ary, indices_or_sections, axis)\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[0;31m# handle array case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 772\u001b[0;31m         \u001b[0mNsections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_or_sections\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    773\u001b[0m         \u001b[0mdiv_points\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_or_sections\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mNtotal\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'int' has no len()",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-ef44e8ce309b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mto_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"audio.mov\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mset_timeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-1cf46805e3fa>\u001b[0m in \u001b[0;36mto_text\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0msections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0msection_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msection\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msections\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36marray_split\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/shape_base.py\u001b[0m in \u001b[0;36marray_split\u001b[0;34m(ary, indices_or_sections, axis)\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0mNsections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_or_sections\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mNsections\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'number sections must be larger than 0.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m         \u001b[0mNeach_section\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextras\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdivmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNsections\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         section_sizes = ([0] +\n",
            "\u001b[0;31mValueError\u001b[0m: number sections must be larger than 0."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSoGeI0jwZCq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}